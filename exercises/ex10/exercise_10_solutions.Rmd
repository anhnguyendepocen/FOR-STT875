---
title: "FOR/STT 875, Exercise 10"
output: html_document
---

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(comment = NA, tidy = TRUE)
```

## Learning objectives
  + Translate statistical notation into coded functions
  + Learn about tools for checking the validity of function arguments
  + Practice writing functions that return multiple objects
  
## Overview
This exercise uses statistical distributions to motivate learning about functions and provide some practice converting algorithms into code. You don't need to know anything about statistics to successfully complete this exercise &#9786;.

## Submission instructions
Upload your exercise_10.Rmd and exercise_10.html files to the Exercise 10 D2L dropbox. Note, I don't provide an exercise_10.Rmd template so you'll need to create your own and populate it with the `# TODO:` code chunks.

## Grading
You will receive full credit if your R Markdown document: 1) compiles without error; and 2) correctly completes the `##TODO:` code chunks. Also, please, fill in the feedback [questions](#questions) at the end of the exercise. 

## Getting started
R provides functions that return useful characteristics of many common statistical distributions. The naming convention for these functions is a prefix, which identifies what the function does, followed by an abbreviation of the statistical distribution's name. These prefixes are: 

  * `p` for "probability", the cumulative distribution function (CDF)
  * `q` for "quantile", the inverse CDF
  * `d` for "density", the density function (PDF)
  * `r` for "random", a random variable having the specified distribution.

For the normal distribution, these functions are `pnorm`, `qnorm`, `dnorm`, and `rnorm` (where the `norm` portion reminds us this is for the normal distribution). For the binomial distribution, these functions are `pbinom`, `qbinom`, `dbinom`, and `rbinom`. And so forth. If you're curious, [here's](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Distributions.html) a list of statistical distributions in base R

The Pareto distribution is not available in base R, so we're going to code it up ourselves! Specifically, for this exercise, we'll just code the quantile function, i.e., `qpareto`. Here's a bit of background on deriving the Pareto's quantile function (you don't need to understand the subsequent PDF or CDF details, but pay close attention to the definition of quantile function $Q(p)$, i.e., the last formula).

The Pareto family of distributions is parameterized by $\alpha$ and $x_0$, and has probability density function (PDF)
\[
f(x) = \begin{cases}
\frac{(\alpha - 1)x_0^{\alpha - 1}}{x^{\alpha}}, &x > x_0\\
0, &x \leq x_0.
\end{cases}
\]

From the PDF it is relatively easy to compute the CDF, which is given by
\[
F(x) = \begin{cases}
0 & x < x_0\\
1 - \left(\frac{x_0}{x} \right)^{\alpha - 1} & x \geq x_0.
\end{cases}
\]

The **quantile function** is defined for $0 \le p \le 1$ and returns the value $x_p$ such that $F(x_p) = p$, i.e., the value with area $p$ to its left. For the Pareto distribution the quantile function is given by 
\[
Q(p) = Q(p, \alpha, x_0) = {x_0}{(1-p)^{-\frac{1}{\alpha - 1}}}.
\]

Using the definition of $Q(p)$, it is easy to compute the $p$th quantile for specific values of $p$. For example, here are the medians ($0.5$ quantiles) of Pareto distributions with $x_0 = 1, \alpha = 3.5$, $x_0 = 6\times 10^8, \alpha = 2.34$, and the $0.92$ quantile of the Pareto distribution with $x_0 = 1\times 10^6, \alpha = 2.5$.
```{r}
1*(1-0.5)^(-1/(3.5-1))
6e8 * (1-0.5)^(-1/(2.34-1))
1e6 * (1-0.92)^(-1/(2.5-1))
```

Of course it would be helpful to have a function that automated this process, both so we don't have to remember the form of the quantile function for the Pareto distribution and so we avoid making mistakes. 

## Part 1
Write a function called `qpareto.1` that takes arguments `p`, `alpha`, and `x0` and returns $Q(p, \alpha, x_0)$ (defined above). 

```
## TODO 10.1: write your `qpareto.1` function and show that it returns the same answers as the three tests below
```

```{r, echo=TRUE}
qpareto.1 <- function(p, alpha, x0) {
q <- x0*((1-p)^(-1/(alpha-1)))
return(q)
}
```

```{r}
qpareto.1(p = 0.5, alpha = 3.5, x0 = 1)
qpareto.1(p = 0.5, alpha = 2.34, x0 = 6e8)
qpareto.1(p = 0.92, alpha = 2.5, x0 = 1e6)
```

## Part 2
Most of the quantile functions in R have an argument `lower.tail` that is either `TRUE` or `FALSE`. If `TRUE` the function returns the $p$th quantile. If `FALSE` the function returns the $1-p$th quantile, i.e., returns the value $x_p$ such that $F(x_p) = 1 - p$. This just provides a convenience for the user. 

Create a function `qpareto.2` that has an additional argument `lower.tail` which is by default set to `TRUE`. Your `qpareto.2` function should test whether `lower.tail` is `FALSE`. If it is `FALSE` the function should replace $p$ by $1-p$. Then pass either $p$ or $1-p$ to `qpareto.1` to compute the appropriate quantile, i.e., `qpareto.1` is called from inside of `qpareto.2`.

```{r echo=TRUE}
qpareto.2 <- function(p, alpha, x0, lower.tail=TRUE) {
if(lower.tail==FALSE) {
p <- 1-p
}
q <- qpareto.1(p, alpha, x0)
return(q)
}
```

```
## TODO 10.2: write your `qpareto.2` function and shows that it returns the same answers as the two tests below
```

```{r}
qpareto.2(p = 0.5, alpha = 3.5, x0 = 1)
qpareto.2(p = 0.08, alpha = 2.5, x0 = 1e6, lower.tail = FALSE)
```

Note that there is a downside to writing the function the way we have. We need `qpareto.1` to be in the workspace when `qpareto.2` is called. But there is a big advantage. If we discover a better way to calculate quantiles of the Pareto distribution we can rewrite `qpareto.1` and the new version will automatically be used in `qpareto.2`.

## Part 3
Next, adding checks to ensure that arguments to the function are reasonable would be helpful. In the case of the Pareto quantile function we need $0\leq p\leq 1$, $\alpha > 1$, and $x_0 > 0$. We can use several `if` statements and `stop` functions to check arguments and to display error messages. Another option is to use the `stopifnot` function. This function evaluates each of the expressions given as arguments. If any are not `TRUE`, the `stop` function is called, and a message is printed about the first untrue statement. Here is an example.

```{r, error=TRUE}
ff <- function(p, y, z){
stopifnot(p > 0, p < 1, y < z)
return(c(p, y, z))
}

ff(p = 0.5, y = 3, z = 5)
ff(p = -1, y = 3, z = 5)
ff(p = -1, y = 3, z = 2)
ff(p = 2, y = 3, z = 5)
ff(p = 0.5, y = 3, z = 2)
```

Write a function `qpareto` that adds a `stopifnot` statement to the `qpareto.2` function. The `stopifnot` statement should check the validity of the three arguments `p`, `x0`, and `alpha`.

Importantly, R Markdown will not compile if your R function stops due to `stopifnot` function. You can, and should, tell the offending R code chunks to ignore the stop call, by including `error=TRUE` in the code chunk's opening tag (see point \#5 [here](http://yihui.name/knitr/demo/output) in the `knitr` help page).

```{r, echo=TRUE}
qpareto <- function(p, alpha, x0, lower.tail=TRUE) {
stopifnot(p >= 0, p <= 1, alpha> 1, x0 > 0)
if(lower.tail==FALSE) {
p <- 1-p
}
q <- qpareto.1(p, alpha, x0)
return(q)
}
``` 

```
## TODO 10.3: write your `qpareto` function and shows that it returns the same answers as the five tests below

```

```{r, error=TRUE}
qpareto(p = 0.5, alpha = 3.5, x0 = 1)
qpareto(p = 0.08, alpha = 2.5, x0 = 1e6, lower.tail = FALSE)
qpareto(p = 1.08, alpha = 2.5, x0 = 1e6, lower.tail = FALSE)
qpareto(p = 0.5, alpha = 0.5, x0 = -4)
qpareto(p = 0.5, alpha = 2, x0 = -4)
```

## Part 4

The `qpareto` functions returned a length one vector. Often functions should return more complex R objects such as lists. The maximum likelihood estimators of the mean and variance of a normal distribution are the sample mean ($\hat \mu$) and a scaled version of the sample variance ($\hat\sigma^2$) defined as:
\[
\hat \mu = \frac{\sum_{i=1}^n x_i}{n} \quad\text{and}\quad \hat\sigma^2 = \frac{\sum_{i=1}^n (x_i - \hat \mu)^2}{n}.
\]

Write a function named `normal.mle` that takes as input a vector `x` of data values and returns a two-component list. One component should be named `mean_hat` and should be the estimate of the mean. The other component should be named `var_hat` and should be the estimate of the variance. The function should check whether the argument `x` is numeric, and whether the length of `x` is at least two. (Hint: you'll want to use the `sum` function in both your mean and variance expressions).

```{r echo=TRUE}
normal.mle <- function(x) {
stopifnot(is.numeric(x), length(x) >=2)
n <- length(x)
mean_hat <- mean(x)
var_hat  <- var(x)*(n-1)/n
out <- list(mean_hat=mean_hat, var_hat=var_hat)
return(out)
}

normal.mle <- function(x, n) {
  mean_hat <- sum(x/n)
  var_hat <- sum((x-mean_hat^2)/n)
  return(mean_hat, var_hat)
}
```

```
## TODO 10.4: write your `normal.mle` function and shows that it returns the same answers as the three tests below
```

```{r, error=TRUE}
normal.mle(c(1, 2, 1, 4))
normal.mle(c("a", "b"))
normal.mle(1)
sessionInfo()
```

******

## Challenge Question

This question **will not be graded** and is provided for additional practice building your `R` skills. This is a *challenge* question, and thus not all material has been (or will be) covered. 

In this question, you will essentially recreate a basic version of the `lm` function for fitting a basic linear model via *Ordinary Least Squares*. Your function will not return the same output as the `lm` function, and rather will simply return the OLS estimate for the regression coefficients, as well as the standard errors for those regression coefficients in a list.  

The formula for the OLS estimate for the regression coefficients is as follows: 

$$\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}\boldsymbol{X}^{\text{T}}\boldsymbol{Y}$$

where $\boldsymbol{X}^{\text{T}}$ denotes the transpose of $\boldsymbol{X}$, $\boldsymbol{X}$ is the design matrix, and $\boldsymbol{Y}$ is the data.  

The formula for the standard error of the OLS estimates of the regression coefficients is as follows: 

$$
\frac{(\boldsymbol{Y} - \boldsymbol{X\hat{\beta}})^{\text{T}}(\boldsymbol{Y} - \boldsymbol{X\hat{\beta}})}{n - p} \times (\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}
$$

where $n$ is the number of data points and $p$ is the number of parameters to be estimated (including the intercept). In our specific case detailed below, $n = 100$ and $p = 2$. 

Check your function with the following simulated data, and ensure you can recover the original parameter values used in the simulation, and that these match the values given from the built-in `lm` function .

```{r}
set.seed(12345)
x <- rnorm(100,160,sd = 15)
y <- 80+1.02*(x) + rnorm(100, 0, sd = 1)
# Create design matrix
X <- cbind(1, x)
```

```{r, echo = TRUE}
my.ols <- function(y, x) {
  beta.hat <- solve(t(x) %*% x) %*% (t(x) %*% y)
  pred <- as.vector(X %*% beta.hat)
  e <- y - pred
  n <- length(y)
  p <- dim(X)[2]
  VC <- (as.vector((t(e) %*% e)) / (n-p)) * solve(t(X) %*% X)
  standard.errors <- sqrt(diag(VC))
  return(list(estimate = as.vector(beta.hat), std.error = standard.errors))
}
```

I called my function `my.ols` and it takes the data vector `y` as input, as well as the design matrix `X`. My function produces the following results: 

```{r}
my.ols(y, X)
```

We can now look at the `lm` function output, which we see gives the same results for the coefficient estimates and their standard errors

```{r}
summary(lm(y ~ X - 1))
```

We used the following functions in our function: `solve` (take the inverse of a matrix), `%*%` (matrix multiplication), `as.vector`, `length`, `dim`, `sqrt`, `diag` (obtain diagonal elements from a matrix). 

Congratulations! You've reached the end of Exercise 10.

### Questions?

If you have any lingering questions about the material in this document or in the corresponding chapter, put them here.

*Response...*


### Give us your feedback!

1.  How do you feel you're doing in this class?

*Response...*

2.  What could be done to improve your learning experience?

*Response...*

*******
